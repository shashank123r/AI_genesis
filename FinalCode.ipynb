{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivanihmm/AI_genesis/blob/main/Miniprojectfinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixIRipj3HifZ",
        "outputId": "e46df642-e690-43a1-f981-4ef549697df0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.30.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.1 (from gradio)\n",
            "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.30.0-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.30.0 gradio-client-1.10.1 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.10 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.59)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.42)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (2.11.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.24 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain_groq) (0.3.59)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.25.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.13.2)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_groq) (0.3.42)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_groq) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_groq) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_groq) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_groq) (24.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain_groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain_groq) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain_groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain_groq) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain_groq) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain_groq) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain_groq) (2.4.0)\n",
            "Downloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading groq-0.25.0-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain_groq\n",
            "Successfully installed groq-0.25.0 langchain_groq-0.3.2\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (11.2.1)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading XlsxWriter-3.2.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (4.13.2)\n",
            "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading XlsxWriter-3.2.3-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-3.2.3 python-pptx-1.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio\n",
        "!pip install langchain_community\n",
        "!pip install langchain_groq\n",
        "!pip install -q gradio langchain langchain-community langchain-groq faiss-cpu sentence-transformers pypdf python-docx\n",
        "!pip install PyPDF2\n",
        "!pip install python-pptx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsXYrzrrHPvw"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import tempfile\n",
        "import shutil\n",
        "import requests\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "import gradio as gr\n",
        "\n",
        "# Import required libraries here to avoid imports in Google Colab\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Add Groq integration\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Import document processing libraries\n",
        "from docx import Document\n",
        "from pptx import Presentation\n",
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ajKZuBBHLu5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class SmartStudyAI:\n",
        "    \"\"\"\n",
        "    A study assistant that processes PDFs, answers questions, and generates\n",
        "    study materials like flashcards and practice exams.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        pdf_directory: str = \"pdfs\",\n",
        "        db_path: str = \"studybuddy_vectordb\",\n",
        "        llm_type: str = \"groq\",  # \"ollama\" or \"groq\"\n",
        "        model_name: str = \"llama3-70b-8192\",\n",
        "        api_key: str = None\n",
        "    ):\n",
        "        # Setup directories\n",
        "        self.pdf_directory = pdf_directory\n",
        "        self.db_path = db_path\n",
        "        self.api_key = api_key\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # Create directories if they don't exist\n",
        "        for directory in [pdf_directory, \"flashcards\", \"exams\"]:\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "\n",
        "        # Initialize components\n",
        "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        )\n",
        "        self.vectorstore = None\n",
        "\n",
        "        # Initialize LLM based on type\n",
        "        if llm_type == \"groq\" and api_key:\n",
        "            from langchain_groq import ChatGroq\n",
        "            self.llm = ChatGroq(\n",
        "                api_key=api_key,\n",
        "                model_name=model_name,\n",
        "                temperature=0.2\n",
        "            )\n",
        "        else:\n",
        "            # Default to Ollama\n",
        "            from langchain_community.llms import Ollama\n",
        "            self.llm = Ollama(model=model_name, temperature=0.2)\n",
        "\n",
        "        self.qa_chain = None\n",
        "\n",
        "        # Try to load existing vectorstore\n",
        "        if os.path.exists(self.db_path):\n",
        "            self.load_vectorstore()\n",
        "            self.setup_qa_chain()\n",
        "\n",
        "    def load_and_process_pdfs(self, force_reload: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Load all PDFs from the directory, process them, and create a vector database.\n",
        "\n",
        "        Args:\n",
        "            force_reload: If True, reprocess PDFs even if database exists\n",
        "        \"\"\"\n",
        "        if os.path.exists(self.db_path) and not force_reload:\n",
        "            print(f\"Vector database already exists at {self.db_path}. Loading...\")\n",
        "            self.load_vectorstore()\n",
        "            return\n",
        "\n",
        "        # Check if directory exists and has PDFs\n",
        "        if not os.path.exists(self.pdf_directory):\n",
        "            print(f\"Directory {self.pdf_directory} does not exist. Creating it...\")\n",
        "            os.makedirs(self.pdf_directory)\n",
        "            print(f\"Please add PDF files to {self.pdf_directory} and run again.\")\n",
        "            return\n",
        "\n",
        "        pdf_files = [f for f in os.listdir(self.pdf_directory) if f.endswith('.pdf')]\n",
        "        if not pdf_files:\n",
        "            print(f\"No PDF files found in {self.pdf_directory}. Please add some PDFs.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Processing {len(pdf_files)} PDF files...\")\n",
        "        documents = []\n",
        "\n",
        "        # Process each PDF\n",
        "        for filename in pdf_files:\n",
        "            file_path = os.path.join(self.pdf_directory, filename)\n",
        "            print(f\"Loading {filename}...\")\n",
        "            try:\n",
        "                loader = PyPDFLoader(file_path)\n",
        "                docs = loader.load()\n",
        "\n",
        "                # Add source metadata to each document\n",
        "                for doc in docs:\n",
        "                    doc.metadata['source'] = filename\n",
        "                    # Ensure page is an integer\n",
        "                    if 'page' not in doc.metadata:\n",
        "                        doc.metadata['page'] = 1\n",
        "\n",
        "                documents.extend(docs)\n",
        "                print(f\"  Added {len(docs)} pages from {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing {filename}: {str(e)}\")\n",
        "\n",
        "        if not documents:\n",
        "            print(\"No documents were successfully processed.\")\n",
        "            return\n",
        "\n",
        "        # Split documents into chunks\n",
        "        print(\"Splitting documents into chunks...\")\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,\n",
        "            chunk_overlap=50\n",
        "        )\n",
        "        chunks = text_splitter.split_documents(documents)\n",
        "        print(f\"Created {len(chunks)} chunks from {len(documents)} pages.\")\n",
        "\n",
        "        # Create vector store\n",
        "        print(\"Creating vector embeddings (this may take a while)...\")\n",
        "        self.vectorstore = FAISS.from_documents(chunks, self.embeddings)\n",
        "        self.vectorstore.save_local(self.db_path)\n",
        "        print(f\"Vector database created and saved to {self.db_path}\")\n",
        "\n",
        "        # Setup QA chain\n",
        "        self.setup_qa_chain()\n",
        "\n",
        "    def load_vectorstore(self) -> None:\n",
        "        \"\"\"Load the vector database from disk\"\"\"\n",
        "        try:\n",
        "            self.vectorstore = FAISS.load_local(\n",
        "                self.db_path,\n",
        "                self.embeddings,\n",
        "                allow_dangerous_deserialization=True\n",
        "            )\n",
        "            print(\"Vector database loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading vector database: {str(e)}\")\n",
        "            print(\"You may need to reprocess your PDFs.\")\n",
        "\n",
        "    def setup_qa_chain(self) -> None:\n",
        "        \"\"\"Initialize the question-answering chain\"\"\"\n",
        "        if self.vectorstore is None:\n",
        "            print(\"Vector database not loaded. Please load or process PDFs first.\")\n",
        "            return\n",
        "\n",
        "        retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "        self.qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=retriever,\n",
        "            return_source_documents=True\n",
        "        )\n",
        "        print(\"QA chain initialized and ready for queries.\")\n",
        "\n",
        "    def query(self, question: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Ask a question about the content in the processed PDFs\n",
        "\n",
        "        Args:\n",
        "            question: The question to ask\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with answer and source information\n",
        "        \"\"\"\n",
        "        if self.qa_chain is None:\n",
        "            return {\n",
        "                \"answer\": \"QA system not initialized. Please process PDFs first.\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            result = self.qa_chain({\"query\": question})\n",
        "\n",
        "            # Extract source information\n",
        "            sources = []\n",
        "            for doc in result['source_documents']:\n",
        "                sources.append({\n",
        "                    \"content\": doc.page_content,\n",
        "                    \"source\": doc.metadata.get('source', 'Unknown'),\n",
        "                    \"page\": doc.metadata.get('page', 1)\n",
        "                })\n",
        "\n",
        "            return {\n",
        "                \"answer\": result['result'],\n",
        "                \"sources\": sources\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"answer\": f\"Error: {str(e)}\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n",
        "    def get_summary(self, topic: str = \"\") -> str:\n",
        "        \"\"\"Generate a summary of content related to a topic\"\"\"\n",
        "        if self.vectorstore is None:\n",
        "            return \"Vector database not loaded. Please process PDFs first.\"\n",
        "\n",
        "        # If topic is provided, search for relevant documents\n",
        "        if topic:\n",
        "            docs = self.vectorstore.similarity_search(topic, k=8)\n",
        "            context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "            prompt = f\"Generate a comprehensive summary about '{topic}' based on the following extracted content:\\n\\n{context}\"\n",
        "        else:\n",
        "            # For general summary, use more documents to get broader coverage\n",
        "            docs = self.vectorstore.similarity_search(\"important concepts and key ideas\", k=15)\n",
        "            context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "            prompt = \"Generate a comprehensive summary of the main topics and key concepts in the documents based on the following extracted content:\\n\\n\" + context\n",
        "\n",
        "        try:\n",
        "            # Using Groq API directly for summary generation\n",
        "            headers = {\n",
        "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "                \"Content-Type\": \"application/json\",\n",
        "            }\n",
        "\n",
        "            payload = {\n",
        "                \"model\": self.model_name,\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant who creates concise but comprehensive summaries.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                \"max_tokens\": 1500,\n",
        "                \"temperature\": 0.5,\n",
        "            }\n",
        "\n",
        "            response = requests.post(\"https://api.groq.com/openai/v1/chat/completions\",\n",
        "                                    headers=headers,\n",
        "                                    json=payload)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "            else:\n",
        "                return f\"Error generating summary: {response.text}\"\n",
        "        except Exception as e:\n",
        "            return f\"Error generating summary: {str(e)}\"\n",
        "\n",
        "    def extract_text_from_file(self, file_path: str) -> str:\n",
        "        \"\"\"Extract text from various file formats\"\"\"\n",
        "        try:\n",
        "            name = file_path.lower()\n",
        "            if name.endswith(\".pdf\"):\n",
        "                reader = PdfReader(file_path)\n",
        "                return \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
        "            elif name.endswith(\".docx\"):\n",
        "                doc = Document(file_path)\n",
        "                return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "            elif name.endswith(\".pptx\"):\n",
        "                prs = Presentation(file_path)\n",
        "                text = []\n",
        "                for slide in prs.slides:\n",
        "                    for shape in slide.shapes:\n",
        "                        if hasattr(shape, \"text\"):\n",
        "                            text.append(shape.text)\n",
        "                return \"\\n\".join(text)\n",
        "            else:\n",
        "                return \"\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from {file_path}: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def generate_flashcards_from_file(self, file_path: str, num_cards: int = 10) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Generate flashcards from an uploaded file (PDF, DOCX, PPTX)\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the file\n",
        "            num_cards: Number of flashcards to generate\n",
        "\n",
        "        Returns:\n",
        "            List of flashcard dictionaries\n",
        "        \"\"\"\n",
        "        text = self.extract_text_from_file(file_path)\n",
        "        if not text or not text.strip():\n",
        "            return []\n",
        "\n",
        "        return self.generate_flashcards_with_text(text, num_cards=num_cards)\n",
        "\n",
        "    def generate_flashcards_with_text(self, text: str, topic: str = \"\", num_cards: int = 10) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Generate flashcards using the Groq API from text\n",
        "\n",
        "        Args:\n",
        "            text: Text content to generate flashcards from\n",
        "            topic: Optional topic to focus on\n",
        "            num_cards: Number of flashcards to generate\n",
        "\n",
        "        Returns:\n",
        "            List of flashcard dictionaries\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if topic:\n",
        "                prompt = f\"Based on the following content, generate {num_cards} high-quality study flashcards about '{topic}'. For each flashcard, include a question, answer, and difficulty level (easy/medium/hard).\\n\\nContent:\\n{text}\"\n",
        "            else:\n",
        "                prompt = f\"Based on the following content, generate {num_cards} high-quality study flashcards covering the key concepts. For each flashcard, include a question, answer, and difficulty level (easy/medium/hard).\\n\\nContent:\\n{text}\"\n",
        "\n",
        "            headers = {\n",
        "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "                \"Content-Type\": \"application/json\",\n",
        "            }\n",
        "\n",
        "            payload = {\n",
        "                \"model\": self.model_name,\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant who generates educational flashcards.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                \"max_tokens\": 2048,\n",
        "                \"temperature\": 0.7,\n",
        "            }\n",
        "\n",
        "            response = requests.post(\"https://api.groq.com/openai/v1/chat/completions\",\n",
        "                                    headers=headers,\n",
        "                                    json=payload)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "                flashcards = []\n",
        "\n",
        "                # Try to parse the response into our flashcard format\n",
        "                # First, try to see if it's already formatted as we expect\n",
        "                if \"Q:\" in content or \"Question:\" in content:\n",
        "                    lines = content.split(\"\\n\")\n",
        "                    current_card = {}\n",
        "\n",
        "                    for line in lines:\n",
        "                        line = line.strip()\n",
        "                        if not line:\n",
        "                            continue\n",
        "\n",
        "                        if line.startswith(\"Q:\") or line.startswith(\"Question:\"):\n",
        "                            if \"question\" in current_card:\n",
        "                                flashcards.append(current_card)\n",
        "                                current_card = {}\n",
        "                            current_card[\"question\"] = line.split(\":\", 1)[1].strip()\n",
        "                            current_card[\"difficulty\"] = \"medium\"  # Default difficulty\n",
        "                        elif line.startswith(\"A:\") or line.startswith(\"Answer:\"):\n",
        "                            current_card[\"answer\"] = line.split(\":\", 1)[1].strip()\n",
        "                        elif \"Difficulty:\" in line or \"Level:\" in line:\n",
        "                            difficulty_part = line.split(\":\", 1)[1].strip().lower()\n",
        "                            if \"easy\" in difficulty_part:\n",
        "                                current_card[\"difficulty\"] = \"easy\"\n",
        "                            elif \"hard\" in difficulty_part:\n",
        "                                current_card[\"difficulty\"] = \"hard\"\n",
        "                            else:\n",
        "                                current_card[\"difficulty\"] = \"medium\"\n",
        "\n",
        "                    if \"question\" in current_card and \"answer\" in current_card:\n",
        "                        flashcards.append(current_card)\n",
        "                else:\n",
        "                    # Try to extract flashcards from paragraphs\n",
        "                    sections = content.split(\"---\")\n",
        "                    for section in sections:\n",
        "                        if not section.strip():\n",
        "                            continue\n",
        "\n",
        "                        card = {\"difficulty\": \"medium\"}  # Default difficulty\n",
        "\n",
        "                        # Try to find question and answer\n",
        "                        q_matches = [\"Question:\", \"Q:\"]\n",
        "                        a_matches = [\"Answer:\", \"A:\"]\n",
        "\n",
        "                        for line in section.split(\"\\n\"):\n",
        "                            line = line.strip()\n",
        "                            if not line:\n",
        "                                continue\n",
        "\n",
        "                            # Check for question\n",
        "                            for q_match in q_matches:\n",
        "                                if q_match in line:\n",
        "                                    card[\"question\"] = line.split(q_match, 1)[1].strip()\n",
        "                                    break\n",
        "\n",
        "                            # Check for answer\n",
        "                            for a_match in a_matches:\n",
        "                                if a_match in line:\n",
        "                                    card[\"answer\"] = line.split(a_match, 1)[1].strip()\n",
        "                                    break\n",
        "\n",
        "                            # Check for difficulty\n",
        "                            if \"difficulty\" in line.lower() or \"level\" in line.lower():\n",
        "                                if \"easy\" in line.lower():\n",
        "                                    card[\"difficulty\"] = \"easy\"\n",
        "                                elif \"hard\" in line.lower():\n",
        "                                    card[\"difficulty\"] = \"hard\"\n",
        "                                else:\n",
        "                                    card[\"difficulty\"] = \"medium\"\n",
        "\n",
        "                        if \"question\" in card and \"answer\" in card:\n",
        "                            flashcards.append(card)\n",
        "\n",
        "                # If we couldn't parse any flashcards, create some manually\n",
        "                if not flashcards:\n",
        "                    # Make some basic cards from the content\n",
        "                    parts = content.split(\"\\n\\n\")\n",
        "                    for i, part in enumerate(parts):\n",
        "                        if len(part.strip()) > 10:  # Only use substantial parts\n",
        "                            # Create a basic flashcard\n",
        "                            question = f\"What is important to understand about {topic if topic else 'this content'} (part {i+1})?\"\n",
        "                            flashcards.append({\n",
        "                                \"question\": question,\n",
        "                                \"answer\": part.strip(),\n",
        "                                \"difficulty\": \"medium\"\n",
        "                            })\n",
        "\n",
        "                            # Limit to requested number\n",
        "                            if len(flashcards) >= num_cards:\n",
        "                                break\n",
        "\n",
        "                # Limit to requested number\n",
        "                return flashcards[:num_cards]\n",
        "            else:\n",
        "                print(f\"API Error: {response.text}\")\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating flashcards: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def generate_flashcards(self, topic: str, num_cards: int = 10) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Generate flashcards about a topic from the processed PDFs\n",
        "\n",
        "        Args:\n",
        "            topic: The topic to create flashcards for\n",
        "            num_cards: Number of flashcards to generate\n",
        "\n",
        "        Returns:\n",
        "            List of flashcard dictionaries\n",
        "        \"\"\"\n",
        "        if self.vectorstore is None:\n",
        "            print(\"Vector database not loaded. Please process PDFs first.\")\n",
        "            return []\n",
        "\n",
        "        # Retrieve relevant documents for the topic\n",
        "        docs = self.vectorstore.similarity_search(topic, k=8)\n",
        "        content = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        try:\n",
        "            # Using Groq API directly for flashcard generation\n",
        "            prompt = f\"Based on the following content, generate {num_cards} high-quality study flashcards about '{topic}'. Format each flashcard with a question, answer, and difficulty level (easy/medium/hard).\\n\\nContent:\\n{content}\"\n",
        "\n",
        "            headers = {\n",
        "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "                \"Content-Type\": \"application/json\",\n",
        "            }\n",
        "\n",
        "            payload = {\n",
        "                \"model\": self.model_name,\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant who generates educational flashcards.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                \"max_tokens\": 2048,\n",
        "                \"temperature\": 0.7,\n",
        "            }\n",
        "\n",
        "            response = requests.post(\"https://api.groq.com/openai/v1/chat/completions\",\n",
        "                                    headers=headers,\n",
        "                                    json=payload)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "                flashcards = []\n",
        "\n",
        "                # Try to parse the response into our flashcard format\n",
        "                # First, try to see if it's already formatted as we expect\n",
        "                if \"Q:\" in content or \"Question:\" in content:\n",
        "                    lines = content.split(\"\\n\")\n",
        "                    current_card = {}\n",
        "\n",
        "                    for line in lines:\n",
        "                        line = line.strip()\n",
        "                        if not line:\n",
        "                            continue\n",
        "\n",
        "                        if line.startswith(\"Q:\") or line.startswith(\"Question:\"):\n",
        "                            if \"question\" in current_card:\n",
        "                                flashcards.append(current_card)\n",
        "                                current_card = {}\n",
        "                            current_card[\"question\"] = line.split(\":\", 1)[1].strip()\n",
        "                            current_card[\"difficulty\"] = \"medium\"  # Default difficulty\n",
        "                        elif line.startswith(\"A:\") or line.startswith(\"Answer:\"):\n",
        "                            current_card[\"answer\"] = line.split(\":\", 1)[1].strip()\n",
        "                        elif \"Difficulty:\" in line or \"Level:\" in line:\n",
        "                            difficulty_part = line.split(\":\", 1)[1].strip().lower()\n",
        "                            if \"easy\" in difficulty_part:\n",
        "                                current_card[\"difficulty\"] = \"easy\"\n",
        "                            elif \"hard\" in difficulty_part:\n",
        "                                current_card[\"difficulty\"] = \"hard\"\n",
        "                            else:\n",
        "                                current_card[\"difficulty\"] = \"medium\"\n",
        "\n",
        "                    if \"question\" in current_card and \"answer\" in current_card:\n",
        "                        flashcards.append(current_card)\n",
        "                else:\n",
        "                    # Try to extract flashcards from paragraphs\n",
        "                    sections = content.split(\"---\")\n",
        "                    for section in sections:\n",
        "                        if not section.strip():\n",
        "                            continue\n",
        "\n",
        "                        card = {\"difficulty\": \"medium\"}  # Default difficulty\n",
        "\n",
        "                        # Try to find question and answer\n",
        "                        q_matches = [\"Question:\", \"Q:\"]\n",
        "                        a_matches = [\"Answer:\", \"A:\"]\n",
        "\n",
        "                        for line in section.split(\"\\n\"):\n",
        "                            line = line.strip()\n",
        "                            if not line:\n",
        "                                continue\n",
        "\n",
        "                            # Check for question\n",
        "                            for q_match in q_matches:\n",
        "                                if q_match in line:\n",
        "                                    card[\"question\"] = line.split(q_match, 1)[1].strip()\n",
        "                                    break\n",
        "\n",
        "                            # Check for answer\n",
        "                            for a_match in a_matches:\n",
        "                                if a_match in line:\n",
        "                                    card[\"answer\"] = line.split(a_match, 1)[1].strip()\n",
        "                                    break\n",
        "\n",
        "                            # Check for difficulty\n",
        "                            if \"difficulty\" in line.lower() or \"level\" in line.lower():\n",
        "                                if \"easy\" in line.lower():\n",
        "                                    card[\"difficulty\"] = \"easy\"\n",
        "                                elif \"hard\" in line.lower():\n",
        "                                    card[\"difficulty\"] = \"hard\"\n",
        "                                else:\n",
        "                                    card[\"difficulty\"] = \"medium\"\n",
        "\n",
        "                        if \"question\" in card and \"answer\" in card:\n",
        "                            flashcards.append(card)\n",
        "\n",
        "                # If we couldn't parse any flashcards, create some manually\n",
        "                if not flashcards:\n",
        "                    # Make some basic cards from the content\n",
        "                    parts = content.split(\"\\n\\n\")\n",
        "                    for i, part in enumerate(parts):\n",
        "                        if len(part.strip()) > 10:  # Only use substantial parts\n",
        "                            # Create a basic flashcard\n",
        "                            question = f\"What is important to understand about {topic} (part {i+1})?\"\n",
        "                            flashcards.append({\n",
        "                                \"question\": question,\n",
        "                                \"answer\": part.strip(),\n",
        "                                \"difficulty\": \"medium\"\n",
        "                            })\n",
        "\n",
        "                            # Limit to requested number\n",
        "                            if len(flashcards) >= num_cards:\n",
        "                                break\n",
        "\n",
        "                # Limit to requested number\n",
        "                return flashcards[:num_cards]\n",
        "            else:\n",
        "                print(f\"API Error: {response.text}\")\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating flashcards: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def generate_study_links(self, topic: str, num_links: int = 5) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Generate study material links from Google Scholar and other sources\n",
        "\n",
        "        Args:\n",
        "            topic: The topic to search for\n",
        "            num_links: Number of links to generate\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries with link information\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Using Groq API to generate search queries and links\n",
        "            prompt = f\"\"\"Generate {num_links} high-quality study resource links about '{topic}'.\n",
        "            For each link, provide:\n",
        "            1. The title of the resource\n",
        "            2. The URL (make sure it's a valid and working URL)\n",
        "            3. The source (Google Scholar,udacity,stanford , Coursera, etc.)\n",
        "            4. A brief description of what the resource contains\n",
        "\n",
        "            Format each link as:\n",
        "            - Title: [title]\n",
        "            - URL: [url]\n",
        "            - Source: [source]\n",
        "            - Description: [description]\"\"\"\n",
        "\n",
        "            headers = {\n",
        "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "                \"Content-Type\": \"application/json\",\n",
        "            }\n",
        "\n",
        "            payload = {\n",
        "                \"model\": self.model_name,\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant who finds educational resources.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                \"max_tokens\": 2048,\n",
        "                \"temperature\": 0.7,\n",
        "            }\n",
        "\n",
        "            response = requests.post(\"https://api.groq.com/openai/v1/chat/completions\",\n",
        "                                    headers=headers,\n",
        "                                    json=payload)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "                links = []\n",
        "\n",
        "                # Parse the response into our link format\n",
        "                current_link = {}\n",
        "                for line in content.split(\"\\n\"):\n",
        "                    line = line.strip()\n",
        "                    if not line:\n",
        "                        continue\n",
        "\n",
        "                    if line.startswith(\"- Title:\"):\n",
        "                        if current_link:\n",
        "                            links.append(current_link)\n",
        "                            current_link = {}\n",
        "                        current_link[\"title\"] = line.split(\":\", 1)[1].strip()\n",
        "                    elif line.startswith(\"- URL:\"):\n",
        "                        url = line.split(\":\", 1)[1].strip()\n",
        "                        # Ensure URL starts with http\n",
        "                        if not url.startswith(\"http\"):\n",
        "                            url = \"https://\" + url\n",
        "                        current_link[\"url\"] = url\n",
        "                    elif line.startswith(\"- Source:\"):\n",
        "                        current_link[\"source\"] = line.split(\":\", 1)[1].strip()\n",
        "                    elif line.startswith(\"- Description:\"):\n",
        "                        current_link[\"description\"] = line.split(\":\", 1)[1].strip()\n",
        "\n",
        "                if current_link and \"url\" in current_link:\n",
        "                    links.append(current_link)\n",
        "\n",
        "                return links[:num_links]\n",
        "            else:\n",
        "                print(f\"API Error: {response.text}\")\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating study links: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "# Set up the Gradio app\n",
        "def create_gradio_interface():\n",
        "    # Initialize global variables\n",
        "    assistant = None\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    pdf_dir = os.path.join(temp_dir, \"pdfs\")\n",
        "    db_dir = os.path.join(temp_dir, \"vectordb\")\n",
        "    os.makedirs(pdf_dir, exist_ok=True)\n",
        "\n",
        "    # For tracking uploaded PDFs\n",
        "    uploaded_pdfs = []\n",
        "\n",
        "    def initialize_assistant(api_key=None):\n",
        "        nonlocal assistant\n",
        "        # Create the AI assistant with Groq by default\n",
        "        llm_type = \"groq\"\n",
        "        model_name = \"llama3-70b-8192\"  # Default Groq model\n",
        "\n",
        "        if not api_key:\n",
        "            return \"Error: API key required for Groq\"\n",
        "\n",
        "        assistant = SmartStudyAI(\n",
        "            pdf_directory=pdf_dir,\n",
        "            db_path=db_dir,\n",
        "            llm_type=\"groq\",\n",
        "            model_name=model_name,\n",
        "            api_key=api_key\n",
        "        )\n",
        "        return f\"Assistant initialized with {llm_type} model: {model_name}\"\n",
        "\n",
        "    def upload_pdf(files):\n",
        "        nonlocal uploaded_pdfs\n",
        "        if not files:\n",
        "            return \"No files uploaded\", uploaded_pdfs\n",
        "\n",
        "        # Save PDFs to directory\n",
        "        for file in files:\n",
        "            # Copy the file to our pdf directory\n",
        "            shutil.copy(file.name, os.path.join(pdf_dir, os.path.basename(file.name)))\n",
        "            uploaded_pdfs.append(os.path.basename(file.name))\n",
        "\n",
        "        return f\"Uploaded {len(files)} PDFs\", uploaded_pdfs\n",
        "\n",
        "    def process_pdfs():\n",
        "        if not assistant:\n",
        "            return \"Please initialize the assistant first!\"\n",
        "\n",
        "        if not uploaded_pdfs:\n",
        "            return \"No PDFs uploaded yet!\"\n",
        "\n",
        "        assistant.load_and_process_pdfs(force_reload=True)\n",
        "        return f\"Processed {len(uploaded_pdfs)} PDFs and created vector database\"\n",
        "\n",
        "    def ask_question(question):\n",
        "        if not assistant:\n",
        "            return \"Please initialize the assistant first!\"\n",
        "\n",
        "        if not question:\n",
        "            return \"Please enter a question!\"\n",
        "\n",
        "        result = assistant.query(question)\n",
        "\n",
        "        answer = result['answer']\n",
        "\n",
        "        # Format sources\n",
        "        if result['sources']:\n",
        "            source_text = \"\\n\\nSources:\\n\"\n",
        "            for i, source in enumerate(result['sources'], 1):\n",
        "                source_text += f\"{i}. {source['source']} (Page {source['page']})\\n\"\n",
        "            answer += source_text\n",
        "\n",
        "        return answer\n",
        "\n",
        "    def generate_summary(topic):\n",
        "        if not assistant:\n",
        "            return \"Please initialize the assistant first!\"\n",
        "\n",
        "        summary = assistant.get_summary(topic)\n",
        "        return summary\n",
        "\n",
        "    def generate_flashcards(topic, num_cards):\n",
        "        if not assistant:\n",
        "            return \"Please initialize the assistant first!\"\n",
        "\n",
        "        try:\n",
        "            flashcards = assistant.generate_flashcards(topic, int(num_cards))\n",
        "            if not flashcards:\n",
        "                return \"Failed to generate flashcards\"\n",
        "\n",
        "            # Format flashcards for display\n",
        "            formatted = f\"# {len(flashcards)} Flashcards on {topic}\\n\\n\"\n",
        "            for i, card in enumerate(flashcards, 1):\n",
        "                formatted += f\"## Card {i} ({card['difficulty']})\\n\"\n",
        "                formatted += f\"**Question:** {card['question']}\\n\\n\"\n",
        "                formatted += f\"**Answer:** {card['answer']}\\n\\n\"\n",
        "                formatted += \"---\\n\\n\"\n",
        "\n",
        "            return formatted\n",
        "        except Exception as e:\n",
        "            return f\"Error generating flashcards: {str(e)}\"\n",
        "\n",
        "    def generate_flashcards_from_file(file, num_cards):\n",
        "        if not assistant:\n",
        "            return \"Please initialize the assistant first!\"\n",
        "\n",
        "        if not file:\n",
        "            return \"No file uploaded for flashcard generation!\"\n",
        "\n",
        "        try:\n",
        "            flashcards = assistant.generate_flashcards_from_file(file.name, int(num_cards))\n",
        "            if not flashcards:\n",
        "                return \"Failed to generate flashcards from file\"\n",
        "\n",
        "            # Format flashcards for display\n",
        "            formatted = f\"# {len(flashcards)} Flashcards from {os.path.basename(file.name)}\\n\\n\"\n",
        "            for i, card in enumerate(flashcards, 1):\n",
        "                formatted += f\"## Card {i} ({card.get('difficulty', 'medium')})\\n\"\n",
        "                formatted += f\"**Question:** {card['question']}\\n\\n\"\n",
        "                formatted += f\"**Answer:** {card['answer']}\\n\\n\"\n",
        "                formatted += \"---\\n\\n\"\n",
        "\n",
        "            return formatted\n",
        "        except Exception as e:\n",
        "            return f\"Error generating flashcards from file: {str(e)}\"\n",
        "\n",
        "    def generate_study_links(topic, num_links):\n",
        "        if not assistant:\n",
        "            return \"Please initialize the assistant first!\"\n",
        "\n",
        "        try:\n",
        "            links = assistant.generate_study_links(topic, int(num_links))\n",
        "            if not links:\n",
        "                return \"Failed to generate study links\"\n",
        "\n",
        "            # Format links as clickable HTML\n",
        "            formatted = f\"<h1>{len(links)} Study Resources on {topic}</h1><br>\"\n",
        "            for i, link in enumerate(links, 1):\n",
        "                formatted += f\"\"\"\n",
        "                <div style='margin-bottom: 20px; border: 1px solid #ddd; padding: 10px; border-radius: 5px;'>\n",
        "                    <h3>{i}. {link.get('title', 'No title')}</h3>\n",
        "                    <p><strong>Source:</strong> {link.get('source', 'Unknown')}</p>\n",
        "                    <p><strong>Description:</strong> {link.get('description', 'No description')}</p>\n",
        "                    <p><a href='{link.get('url', '#')}' target='_blank'>Visit Resource</a></p>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "            return formatted\n",
        "        except Exception as e:\n",
        "            return f\"Error generating study links: {str(e)}\"\n",
        "\n",
        "    # Create the Gradio Interface\n",
        "    with gr.Blocks(title=\"SmartStudyAI\") as app:\n",
        "        gr.Markdown(\"#AI Genisis - Your Personal Study Assistant\")\n",
        "\n",
        "        with gr.Tab(\"Setup\"):\n",
        "            gr.Markdown(\"## Initialize the AI Assistant\")\n",
        "\n",
        "            with gr.Row():\n",
        "                # Hide API key by default and use password input\n",
        "                api_key = gr.Textbox(label=\"API Key (for Groq)\", value=\"Your_Api_key\", type=\"password\", visible=True)\n",
        "                # Set Groq and model as default\n",
        "                model_name = gr.Textbox(label=\"Model Name\", value=\"llama3-70b-8192\", visible=False)\n",
        "\n",
        "            init_btn = gr.Button(\"Initialize Assistant\")\n",
        "            init_output = gr.Textbox(label=\"Initialization Status\")\n",
        "\n",
        "            init_btn.click(initialize_assistant, inputs=[api_key], outputs=init_output)\n",
        "\n",
        "            gr.Markdown(\"## Upload and Process PDFs\")\n",
        "\n",
        "            upload_button = gr.File(label=\"Upload PDFs\", file_count=\"multiple\")\n",
        "            upload_output = gr.Textbox(label=\"Upload Status\")\n",
        "            pdf_list = gr.Dataframe(headers=[\"Uploaded PDFs\"], label=\"Uploaded PDFs\")\n",
        "\n",
        "            process_button = gr.Button(\"Process PDFs\")\n",
        "            process_output = gr.Textbox(label=\"Processing Status\")\n",
        "\n",
        "            upload_button.upload(upload_pdf, inputs=upload_button, outputs=[upload_output, pdf_list])\n",
        "            process_button.click(process_pdfs, outputs=process_output)\n",
        "\n",
        "        with gr.Tab(\"Ask Questions\"):\n",
        "            gr.Markdown(\"## Ask Questions About Your Documents\")\n",
        "\n",
        "            question_input = gr.Textbox(label=\"Your Question\", placeholder=\"What's the main topic of the documents?\")\n",
        "            ask_button = gr.Button(\"Ask\")\n",
        "            answer_output = gr.Markdown(label=\"Answer\")\n",
        "\n",
        "            ask_button.click(ask_question, inputs=question_input, outputs=answer_output)\n",
        "\n",
        "        with gr.Tab(\"Generate Materials\"):\n",
        "            gr.Markdown(\"## Generate Study Materials\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    summary_topic = gr.Textbox(label=\"Topic for Summary (leave empty for general summary)\", placeholder=\"Machine Learning\")\n",
        "                    summary_button = gr.Button(\"Generate Summary\")\n",
        "                    summary_output = gr.Markdown(label=\"Summary\")\n",
        "\n",
        "                    summary_button.click(generate_summary, inputs=summary_topic, outputs=summary_output)\n",
        "\n",
        "                with gr.Column():\n",
        "                    flashcard_topic = gr.Textbox(label=\"Topic for Flashcards\", placeholder=\"Neural Networks\")\n",
        "                    flashcard_num = gr.Number(label=\"Number of Flashcards\", value=5, minimum=1, maximum=20)\n",
        "                    flashcard_button = gr.Button(\"Generate Flashcards\")\n",
        "                    flashcard_output = gr.Markdown(label=\"Flashcards\")\n",
        "\n",
        "                    flashcard_button.click(generate_flashcards, inputs=[flashcard_topic, flashcard_num], outputs=flashcard_output)\n",
        "\n",
        "        with gr.Tab(\"File-Based Flashcards\"):\n",
        "            gr.Markdown(\"## Generate Flashcards from Files (PDF, DOCX, PPTX)\")\n",
        "\n",
        "            with gr.Row():\n",
        "                file_input = gr.File(label=\"Upload File\", file_types=[\".pdf\", \".docx\", \".pptx\"])\n",
        "                file_flashcard_num = gr.Number(label=\"Number of Flashcards\", value=10, minimum=1, maximum=20)\n",
        "\n",
        "            file_flashcard_button = gr.Button(\"Generate Flashcards from File\")\n",
        "            file_flashcard_output = gr.Markdown(label=\"File-Based Flashcards\")\n",
        "\n",
        "            file_flashcard_button.click(\n",
        "                generate_flashcards_from_file,\n",
        "                inputs=[file_input, file_flashcard_num],\n",
        "                outputs=file_flashcard_output\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"Study Resources\"):\n",
        "            gr.Markdown(\"## Find Study Resources Online\")\n",
        "\n",
        "            with gr.Row():\n",
        "                resource_topic = gr.Textbox(label=\"Topic for Resources\", placeholder=\"Machine Learning\")\n",
        "                resource_num = gr.Number(label=\"Number of Resources\", value=5, minimum=1, maximum=10)\n",
        "\n",
        "            resource_button = gr.Button(\"Find Resources\")\n",
        "            resource_output = gr.HTML(label=\"Study Resources\")\n",
        "\n",
        "            resource_button.click(\n",
        "                generate_study_links,\n",
        "                inputs=[resource_topic, resource_num],\n",
        "                outputs=resource_output\n",
        "            )\n",
        "\n",
        "    return app\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 611,
          "referenced_widgets": [
            "9457791c351e4bc2978756fd9ea1ef8f",
            "02da35a4d0ae457ca94f267f6ee941a4",
            "8d0b03e9f9484453ad59a033416a1ed2",
            "7444cfb04a974ceca97a197c26ceb7eb",
            "04a38a94c594441a9965cd79d839d4dc",
            "2174d32ee21f479395f71f66f9497907",
            "38bac37f0f7a4d7bb9a8f0517f2aaa58",
            "6548de179bdd44afa7061817b5e18a99",
            "5af3b4fe919c48728f45c25d5fffceef",
            "c16204f66fce4ce6855aa6d4595f7614",
            "afdd0352a7af48b7afa23ae2a4efefae"
          ]
        },
        "id": "DDNzyrAtHVb4",
        "outputId": "613ef0f2-bac2-48bd-f325-c872db92ade1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://77a7ecc6c9106e4c57.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://77a7ecc6c9106e4c57.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-083c3e660948>:28: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  self.embeddings = HuggingFaceEmbeddings(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9457791c351e4bc2978756fd9ea1ef8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02da35a4d0ae457ca94f267f6ee941a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d0b03e9f9484453ad59a033416a1ed2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7444cfb04a974ceca97a197c26ceb7eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04a38a94c594441a9965cd79d839d4dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2174d32ee21f479395f71f66f9497907",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38bac37f0f7a4d7bb9a8f0517f2aaa58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6548de179bdd44afa7061817b5e18a99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5af3b4fe919c48728f45c25d5fffceef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c16204f66fce4ce6855aa6d4595f7614",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afdd0352a7af48b7afa23ae2a4efefae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 1 PDF files...\n",
            "Loading Afterpulse_and_dark_count_simulator_for_single_pho.pdf...\n",
            "  Added 8 pages from Afterpulse_and_dark_count_simulator_for_single_pho.pdf\n",
            "Splitting documents into chunks...\n",
            "Created 53 chunks from 8 pages.\n",
            "Creating vector embeddings (this may take a while)...\n",
            "Vector database created and saved to /tmp/tmp6uv7f5ry/vectordb\n",
            "QA chain initialized and ready for queries.\n"
          ]
        }
      ],
      "source": [
        "# Launch the app when run directly\n",
        "if __name__ == \"__main__\":\n",
        "    app = create_gradio_interface()\n",
        "    app.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
